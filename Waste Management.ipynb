{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "099e086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22564 images belonging to 2 classes.\n",
      "Found 2513 images belonging to 2 classes.\n",
      "Epoch 1/15\n",
      "706/706 [==============================] - 28s 40ms/step - loss: 0.4462 - accuracy: 0.8023 - val_loss: 0.3829 - val_accuracy: 0.8548\n",
      "Epoch 2/15\n",
      "706/706 [==============================] - 28s 39ms/step - loss: 0.3950 - accuracy: 0.8303 - val_loss: 0.3455 - val_accuracy: 0.8587\n",
      "Epoch 3/15\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.3721 - accuracy: 0.8412 - val_loss: 0.3181 - val_accuracy: 0.8723\n",
      "Epoch 4/15\n",
      "706/706 [==============================] - 28s 40ms/step - loss: 0.3547 - accuracy: 0.8467 - val_loss: 0.3712 - val_accuracy: 0.8361\n",
      "Epoch 5/15\n",
      "706/706 [==============================] - 28s 40ms/step - loss: 0.3421 - accuracy: 0.8564 - val_loss: 0.2818 - val_accuracy: 0.8969\n",
      "Epoch 6/15\n",
      "706/706 [==============================] - 28s 40ms/step - loss: 0.3265 - accuracy: 0.8600 - val_loss: 0.2744 - val_accuracy: 0.8973\n",
      "Epoch 7/15\n",
      "706/706 [==============================] - 30s 43ms/step - loss: 0.3174 - accuracy: 0.8667 - val_loss: 0.2907 - val_accuracy: 0.8882\n",
      "Epoch 8/15\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.3107 - accuracy: 0.8705 - val_loss: 0.2840 - val_accuracy: 0.8882\n",
      "Epoch 9/15\n",
      "706/706 [==============================] - 29s 40ms/step - loss: 0.2997 - accuracy: 0.8739 - val_loss: 0.2422 - val_accuracy: 0.9021\n",
      "Epoch 10/15\n",
      "706/706 [==============================] - 30s 43ms/step - loss: 0.2919 - accuracy: 0.8791 - val_loss: 0.2699 - val_accuracy: 0.8957\n",
      "Epoch 11/15\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.2818 - accuracy: 0.8834 - val_loss: 0.2646 - val_accuracy: 0.8938\n",
      "Epoch 12/15\n",
      "706/706 [==============================] - 30s 42ms/step - loss: 0.2749 - accuracy: 0.8856 - val_loss: 0.2891 - val_accuracy: 0.8866\n",
      "Epoch 13/15\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.2637 - accuracy: 0.8916 - val_loss: 0.3529 - val_accuracy: 0.8496\n",
      "Epoch 14/15\n",
      "706/706 [==============================] - 31s 44ms/step - loss: 0.2522 - accuracy: 0.8967 - val_loss: 0.3019 - val_accuracy: 0.8834\n",
      "Epoch 15/15\n",
      "706/706 [==============================] - 31s 44ms/step - loss: 0.2457 - accuracy: 0.9003 - val_loss: 0.2734 - val_accuracy: 0.8902\n",
      "Prediction: Non-Recyclable Waste\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'DATASET/TRAIN',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'DATASET/TEST',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Building the CNN\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Step 4 - Full Connection\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# Step 5 - Output Layer\n",
    "cnn.add(tf.keras.layers.Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Define a prediction function with tf.function\n",
    "@tf.function(reduce_retracing=True)\n",
    "def predict_with_model(input_tensor):\n",
    "    return cnn(input_tensor)\n",
    "\n",
    "# Training the CNN\n",
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn.fit(x=training_set, validation_data=test_set, epochs=15)\n",
    "\n",
    "# Making Predictions\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('DATASET/TEST/R/R_10015.jpg', target_size=(64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = predict_with_model(test_image)\n",
    "\n",
    "class_indices = training_set.class_indices\n",
    "\n",
    "if result[0][0] > result[0][1]:\n",
    "    prediction = 'Recyclable Waste'\n",
    "else:\n",
    "    prediction = 'Non-Recyclable Waste'\n",
    "\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35adf49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
